{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cats And Dogs Image Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we are building this model with neural networks. Here i am using the keras to do classification. keras is the\n",
    "neural network library which work on top of the Tensorflow or we can say tf as the backend for this library.\n",
    "\n",
    "DATA: we have downloaded our data from the kaggle website you can download it from \n",
    "      here : https://www.kaggle.com/c/dogs-vs-cats/data\n",
    "\n",
    "Here we use simple sequential neural networks which allow us to to use neural nets only layer by layers\n",
    "it does not allow you any complex neural nets such as RNN and LSTM. These neural networks are\n",
    "form simply layers by layes and each layers have their own weights. imput of one layers feeded to another. \n",
    "The sum of the product of current weights and input from previous layers form the activations to next layer.\n",
    "\n",
    "finally in last layer we have flatten these hiddden layers and the last layer is the sigmoid layers which outputs as the\n",
    "Dog or Cat. SIGMOID is generally used for the classification purpose only.\n",
    "\n",
    "The ADAM OPTIMIZER algorithm which  is an extension to stochastic gradient descent.It is useful to give a global\n",
    "optimal solution So we have used it here for optimization.\n",
    "\n",
    "ACTIVATION FUNCTION used is the he Rectified Linear Unit (ReLU) In a neural network,the activation function is responsible\n",
    "for transforming the imputssummed weighted input from the node into the activation of the node or output for that input. \n",
    "\n",
    "To  minimize the loss of the weights and activations we have used the binary_crossentrpy function here.A metric is \n",
    "function that is used to judge the performance of your model. \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMPORTING THE REQUIRED LIBRARIES FIRST\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "# Building the CNN\n",
    "\n",
    "# Importing the Keras libraries and packages'''\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Convolution2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialising the CNN\n",
    "Here i have added various layers. About these layers i discussed above .\n",
    "convolution function takes three arguments the first is about the number of output i. 32 .\n",
    "the filter used here is of size 3X3. the input size of the image is 64x64 pixel the rgb channel used is of size three. Hence the input size becomes as 64x64x3.\n",
    "\n",
    "The maxpooling size is 2x2. i.e from ech matrix of image we wll filter the matrix according to feature with max occurance.\n",
    "the number of layers could be added according to the combination which gives you the best accuracy.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0722 12:41:48.491290 23772 deprecation_wrapper.py:119] From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), input_shape=(64, 64, 3..., activation=\"relu\")`\n",
      "  import sys\n",
      "W0722 12:41:48.539301 23772 deprecation_wrapper.py:119] From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0722 12:41:48.543303 23772 deprecation_wrapper.py:119] From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0722 12:41:48.575310 23772 deprecation_wrapper.py:119] From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:21: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), activation=\"relu\")`\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:27: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"relu\", units=128)`\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:28: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"sigmoid\", units=1)`\n",
      "W0722 12:41:48.663328 23772 deprecation_wrapper.py:119] From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0722 12:41:48.699336 23772 deprecation_wrapper.py:119] From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3376: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "W0722 12:41:48.707339 23772 deprecation.py:323] From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Initialising the CNN\n",
    "classifier = Sequential()\n",
    "\n",
    "# Step 1 - Convolution\n",
    "classifier.add(Convolution2D(32, 3, 3, input_shape = (64, 64, 3), activation = 'relu'))\n",
    "\n",
    "# Step 2 - Pooling\n",
    "classifier.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "\n",
    "# # Adding a second convolutional layer\n",
    "# classifier.add(Convolution2D(32, 3, 3, activation = 'relu'))\n",
    "# classifier.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "\n",
    "# # Adding a third convolutional layer\n",
    "# classifier.add(Convolution2D(32, 3, 3, activation = 'relu'))\n",
    "# classifier.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "\n",
    "# Adding a fourth convolutional layer\n",
    "classifier.add(Convolution2D(32, 3, 3, activation = 'relu'))\n",
    "classifier.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "# Step 3 - Flattening\n",
    "classifier.add(Flatten())\n",
    "\n",
    "# Step 4 - Full connection\n",
    "classifier.add(Dense(output_dim = 128, activation = 'relu'))\n",
    "classifier.add(Dense(output_dim = 1, activation = 'sigmoid'))\n",
    "\n",
    "# Compiling the CNN\n",
    "classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "#Fitting the CNN to the images\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Augmentations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is done so to increase the diversity of data so as to give more insights of the images to the machine algorithms.\n",
    "Here we use shearing , zooming the images and making the horizontal flip of images technique to train our neural nets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale = 1./255,\n",
    "                                   shear_range = 0.2,\n",
    "                                   zoom_range = 0.2,\n",
    "                                   horizontal_flip = True)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale = 1./255)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here i have loaded my data from my local file the data is freely available to kagggle site  there are almost 25000 \n",
    "images of cates and dogs. out of 25000 images we can break it as 15000 for ur training and 10000 as our test datasets.\n",
    "i have used only 8000 images for train and 2000 for test purpose.\n",
    "\n",
    "we will break our images as 64x64 pixels images. and batch_size is the number of images per epoch to be feed to our algo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8000 images belonging to 2 classes.\n",
      "Found 2000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os \n",
    "# os.getcwd(\"C:\\\\Users\\\\Vikas Bhardwaj\\\\Desktop\\\\DATA SCIENCE\\\\Datasets\\\\Datasets\\\\cnn_imgs_dataset\")\n",
    "os.chdir(\"C:\\\\Users\\\\Vikas Bhardwaj\\\\Desktop\\\\DATA SCIENCE\\\\Datasets\\\\Datasets\\\\cnn_imgs_dataset\")\n",
    "\n",
    "\n",
    "training_set = train_datagen.flow_from_directory('training_set',\n",
    "                                                 target_size = (64, 64),\n",
    "                                                 batch_size = 32,\n",
    "                                                 class_mode = 'binary')\n",
    "\n",
    "test_set = test_datagen.flow_from_directory('test_set',\n",
    "                                            target_size = (64, 64),\n",
    "                                            batch_size = 32,\n",
    "                                            class_mode = 'binary')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training our data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have fitted our data to the neural networks.Classifier is the instance of the sequential classifier which\n",
    "is the simplest neural network  which form the layers of nearons.\n",
    "Here we include 100 images per epochs and the total epochs will be 55 validation data is nothing but our test set\n",
    "which consist of 2000 images in our case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:12: UserWarning: The semantics of the Keras 2 argument `steps_per_epoch` is not the same as the Keras 1 argument `samples_per_epoch`. `steps_per_epoch` is the number of batches to draw from the generator at each epoch. Basically steps_per_epoch = samples_per_epoch/batch_size. Similarly `nb_val_samples`->`validation_steps` and `val_samples`->`steps` arguments have changed. Update your method calls accordingly.\n",
      "  if sys.path[0] == '':\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:12: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(<keras_pre..., validation_data=<keras_pre..., steps_per_epoch=250, epochs=10, validation_steps=2000)`\n",
      "  if sys.path[0] == '':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "250/250 [==============================] - 213s 852ms/step - loss: 0.4805 - acc: 0.7688 - val_loss: 0.4743 - val_acc: 0.7737\n",
      "Epoch 2/10\n",
      "250/250 [==============================] - 207s 828ms/step - loss: 0.4717 - acc: 0.7726 - val_loss: 0.4777 - val_acc: 0.7749\n",
      "Epoch 3/10\n",
      "250/250 [==============================] - 210s 840ms/step - loss: 0.4567 - acc: 0.7845 - val_loss: 0.4830 - val_acc: 0.7813\n",
      "Epoch 4/10\n",
      "250/250 [==============================] - 209s 838ms/step - loss: 0.4397 - acc: 0.7923 - val_loss: 0.4563 - val_acc: 0.7917\n",
      "Epoch 5/10\n",
      "250/250 [==============================] - 212s 849ms/step - loss: 0.4201 - acc: 0.8051 - val_loss: 0.4610 - val_acc: 0.7845\n",
      "Epoch 6/10\n",
      "250/250 [==============================] - 212s 848ms/step - loss: 0.4088 - acc: 0.8106 - val_loss: 0.4576 - val_acc: 0.7934\n",
      "Epoch 7/10\n",
      "250/250 [==============================] - 208s 831ms/step - loss: 0.3982 - acc: 0.8183 - val_loss: 0.4755 - val_acc: 0.7868\n",
      "Epoch 8/10\n",
      "250/250 [==============================] - 208s 832ms/step - loss: 0.3851 - acc: 0.8247 - val_loss: 0.4522 - val_acc: 0.7986\n",
      "Epoch 9/10\n",
      "250/250 [==============================] - 208s 834ms/step - loss: 0.3746 - acc: 0.8283 - val_loss: 0.4627 - val_acc: 0.7965\n",
      "Epoch 10/10\n",
      "250/250 [==============================] - 209s 837ms/step - loss: 0.3676 - acc: 0.8345 - val_loss: 0.4767 - val_acc: 0.7904\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1d5d85e9860>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # this will take long time\n",
    "# classifier.fit_generator(training_set,\n",
    "#                          samples_per_epoch = 100,#samples_per_epoch = 8000,\n",
    "#                          nb_epoch = 55, # nb_epoch = 25,\n",
    "#                          validation_data = test_set,\n",
    "#                          nb_val_samples = 200)#nb_val_samples = 2000)\n",
    "\n",
    "classifier.fit_generator(training_set,\n",
    "                         samples_per_epoch = 8000,\n",
    "                         nb_epoch = 10, # nb_epoch = 25,\n",
    "                         validation_data = test_set,\n",
    "                         nb_val_samples = 2000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The accuracy of our model is approx 84% which is quit GOOD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading our single TEST IMAGE  to check if our model gives correct classification or not ?\n",
    "\n",
    "Firstly convert the image to a given pixel size i.e 64x64 as our train images are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=64x64 at 0x2246570C240>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing import image\n",
    "test_image = image.load_img('single_prediction/cat_or_dog_1.jpg', target_size = (64, 64))\n",
    "# This is test img\n",
    "# first arg is the path\n",
    "# img is 64x64 dims this is what v hv used in training so wee need to use exactly the same dims\n",
    "# here also\n",
    "\n",
    "test_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting images to Arrays\n",
    "Converting our test image into  an array becouse machine can understand arrays not images \n",
    "REmenber be have converted our train images also to arrays !!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 54.,  58.,   7.],\n",
       "        [ 58.,  63.,   9.],\n",
       "        [ 64.,  67.,  10.],\n",
       "        ...,\n",
       "        [136., 144.,  71.],\n",
       "        [140., 150.,  77.],\n",
       "        [139., 149.,  78.]],\n",
       "\n",
       "       [[ 48.,  54.,   6.],\n",
       "        [ 51.,  58.,   7.],\n",
       "        [ 58.,  63.,   9.],\n",
       "        ...,\n",
       "        [129., 137.,  64.],\n",
       "        [139., 149.,  78.],\n",
       "        [141., 151.,  80.]],\n",
       "\n",
       "       [[ 48.,  56.,   7.],\n",
       "        [ 48.,  56.,   7.],\n",
       "        [ 54.,  61.,  10.],\n",
       "        ...,\n",
       "        [123., 130.,  63.],\n",
       "        [136., 145.,  80.],\n",
       "        [140., 149.,  82.]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 46.,  55.,  12.],\n",
       "        [ 42.,  50.,   9.],\n",
       "        [ 38.,  49.,   9.],\n",
       "        ...,\n",
       "        [239., 205., 170.],\n",
       "        [235., 209., 186.],\n",
       "        [229., 202., 173.]],\n",
       "\n",
       "       [[ 50.,  57.,  15.],\n",
       "        [ 42.,  50.,   9.],\n",
       "        [ 44.,  52.,  11.],\n",
       "        ...,\n",
       "        [234., 200., 162.],\n",
       "        [236., 206., 178.],\n",
       "        [234., 203., 174.]],\n",
       "\n",
       "       [[ 53.,  59.,  13.],\n",
       "        [ 43.,  51.,  10.],\n",
       "        [ 49.,  56.,  12.],\n",
       "        ...,\n",
       "        [231., 195., 159.],\n",
       "        [235., 213., 190.],\n",
       "        [233., 206., 179.]]], dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_image = image.img_to_array(test_image)\n",
    "# Also in our first layer below it is a 3D array\n",
    "# Step 1 - Convolution\n",
    "# classifier.add(Convolution2D(32, 3, 3, input_shape = (64, 64, 3), activation = 'relu'))\n",
    "# this will convert from a 3D img to 3D array\n",
    "test_image # shld gv us (64,64,3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adjusting the dimension of the image by expanding it'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[ 54.,  58.,   7.],\n",
       "         [ 58.,  63.,   9.],\n",
       "         [ 64.,  67.,  10.],\n",
       "         ...,\n",
       "         [136., 144.,  71.],\n",
       "         [140., 150.,  77.],\n",
       "         [139., 149.,  78.]],\n",
       "\n",
       "        [[ 48.,  54.,   6.],\n",
       "         [ 51.,  58.,   7.],\n",
       "         [ 58.,  63.,   9.],\n",
       "         ...,\n",
       "         [129., 137.,  64.],\n",
       "         [139., 149.,  78.],\n",
       "         [141., 151.,  80.]],\n",
       "\n",
       "        [[ 48.,  56.,   7.],\n",
       "         [ 48.,  56.,   7.],\n",
       "         [ 54.,  61.,  10.],\n",
       "         ...,\n",
       "         [123., 130.,  63.],\n",
       "         [136., 145.,  80.],\n",
       "         [140., 149.,  82.]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 46.,  55.,  12.],\n",
       "         [ 42.,  50.,   9.],\n",
       "         [ 38.,  49.,   9.],\n",
       "         ...,\n",
       "         [239., 205., 170.],\n",
       "         [235., 209., 186.],\n",
       "         [229., 202., 173.]],\n",
       "\n",
       "        [[ 50.,  57.,  15.],\n",
       "         [ 42.,  50.,   9.],\n",
       "         [ 44.,  52.,  11.],\n",
       "         ...,\n",
       "         [234., 200., 162.],\n",
       "         [236., 206., 178.],\n",
       "         [234., 203., 174.]],\n",
       "\n",
       "        [[ 53.,  59.,  13.],\n",
       "         [ 43.,  51.,  10.],\n",
       "         [ 49.,  56.,  12.],\n",
       "         ...,\n",
       "         [231., 195., 159.],\n",
       "         [235., 213., 190.],\n",
       "         [233., 206., 179.]]]], dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_image = np.expand_dims(test_image, axis = 0)\n",
    "# axis specifies the position of indx of the dimnsn v r addng\n",
    "# v need to add the dim in the first position\n",
    "test_image # now it shld show (1,64,64,3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dogs:1 Cats:0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0722 12:42:18.128570 23772 deprecation_wrapper.py:119] From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:2741: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cats': 0, 'dogs': 1}\n"
     ]
    }
   ],
   "source": [
    "result = classifier.predict(test_image)\n",
    "# v r trying to predict\n",
    "result # gv us 1\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "print(training_set.class_indices)\n",
    "\n",
    "if result[0][0] == 1:\n",
    "    prediction = 'dog'\n",
    "else:\n",
    "    prediction = 'cat'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting on test image\n",
    "\n",
    " i am predicting a here on different image which gives us the correct result..\n",
    " As we have given the image of a cat and it is predicting the same.And  the prediction of \n",
    " our model is correct i should be happy to make such a model (LOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dog\n"
     ]
    }
   ],
   "source": [
    "#import numpy as np\n",
    "#from keras.preprocessing import image\n",
    "test_image = image.load_img('single_prediction/cat_or_dog_2.jpg', target_size = (64, 64))\n",
    "test_image = image.img_to_array(test_image)\n",
    "test_image = np.expand_dims(test_image, axis = 0)\n",
    "result = classifier.predict(test_image)\n",
    "training_set.class_indices\n",
    "if result[0][0] == 1:\n",
    "    prediction = 'dog'\n",
    "else:\n",
    "    prediction = 'cat'\n",
    "print(prediction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
